{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097764a5-2de7-4cf8-a042-a884ad7c9be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import time\n",
    "import psutil\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8cff46-7a11-4862-a755-3a00796ab646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'cloud_threshold': 0.3,\n",
    "    'io_workers': min(8, mp.cpu_count()),\n",
    "    'memory_threshold_gb': 2,\n",
    "    'enable_vegetation_indices': True, \n",
    "    'enable_super_resolution': False,  \n",
    "    'enable_advanced_cloud_detection': False,  \n",
    "    'enable_idw_interpolation': False,  \n",
    "    'enable_radiometric_normalization': False,\n",
    "    'downsample_factor': 1,\n",
    "    'max_bands_per_composite': 12,\n",
    "    'skip_quality_checks': True,\n",
    "}\n",
    "\n",
    "# Essential bands only\n",
    "ESSENTIAL_BANDS = ['B02', 'B03', 'B04', 'B08', 'B11']\n",
    "VEGETATION_INDICES = ['EVI', 'GNDVI', 'BSI', 'MSAVI2'] if CONFIG['enable_vegetation_indices'] else []\n",
    "S1_BANDS = ['VV', 'VH']\n",
    "SCL_INVALID = [0, 1, 8, 9, 10, 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee01fd-dd28-4c6b-8ec3-759da8f838c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_system_info():\n",
    "    \"\"\"Get available memory in GB\"\"\"\n",
    "    return psutil.virtual_memory().available / (1024**3)\n",
    "\n",
    "def read_raster(file_path: Path) -> Optional[np.ndarray]:\n",
    "    \"\"\"Read raster with configurable optimizations\"\"\"\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Apply downsampling if configured\n",
    "            downsample = CONFIG['downsample_factor']\n",
    "            if downsample > 1:\n",
    "                out_shape = (src.height // downsample, src.width // downsample)\n",
    "                data = src.read(1, out_shape=out_shape, resampling=rasterio.enums.Resampling.nearest)\n",
    "            else:\n",
    "                # For very large files, auto-downsample\n",
    "                if src.width > 4000 or src.height > 4000:\n",
    "                    out_shape = (src.height // 2, src.width // 2)\n",
    "                    data = src.read(1, out_shape=out_shape, resampling=rasterio.enums.Resampling.nearest)\n",
    "                else:\n",
    "                    data = src.read(1)\n",
    "            \n",
    "            # Convert to float32 efficiently\n",
    "            if data.dtype != np.float32:\n",
    "                data = data.astype(np.float32)\n",
    "            \n",
    "            # Handle nodata efficiently\n",
    "            if src.nodata is not None:\n",
    "                data[data == src.nodata] = np.nan\n",
    "            \n",
    "            return data\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def detect_clouds(scl_data: np.ndarray, bands: Dict[str, np.ndarray] = None) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"Cloud detection with configurable complexity\"\"\"\n",
    "    try:\n",
    "        if CONFIG['enable_advanced_cloud_detection'] and bands is not None:\n",
    "            # Advanced detection\n",
    "            cloud_mask = np.isin(scl_data, SCL_INVALID)\n",
    "            \n",
    "            b02, b04, b08, b11 = [bands.get(b) for b in ['B02', 'B04', 'B08', 'B11']]\n",
    "            \n",
    "            if b02 is not None:\n",
    "                blue_threshold = np.nanpercentile(b02, 85)\n",
    "                cloud_mask |= (b02 > blue_threshold)\n",
    "            \n",
    "            if b08 is not None and b04 is not None:\n",
    "                ndvi = (b08 - b04) / (b08 + b04 + 1e-8)\n",
    "                ndvi_threshold = np.nanpercentile(ndvi, 15)\n",
    "                cloud_mask |= (ndvi < ndvi_threshold)\n",
    "            \n",
    "            if b11 is not None and b02 is not None:\n",
    "                cloud_spectral = (b02 > np.nanpercentile(b02, 75)) & (b11 < np.nanpercentile(b11, 50))\n",
    "                cloud_mask |= cloud_spectral\n",
    "            \n",
    "            # Morphological operations\n",
    "            kernel = np.ones((3, 3), np.uint8)\n",
    "            cloud_mask = ndimage.binary_closing(cloud_mask, structure=kernel)\n",
    "            cloud_mask = ndimage.binary_opening(cloud_mask, structure=kernel)\n",
    "        else:\n",
    "            # Basic detection\n",
    "            if CONFIG['skip_quality_checks']:\n",
    "                # Reduced set for speed\n",
    "                cloud_mask = np.isin(scl_data, [0, 1, 8, 9])\n",
    "            else:\n",
    "                cloud_mask = np.isin(scl_data, SCL_INVALID)\n",
    "        \n",
    "        quality_score = np.sum(~cloud_mask) / cloud_mask.size\n",
    "        return cloud_mask, quality_score\n",
    "        \n",
    "    except Exception:\n",
    "        return np.zeros_like(scl_data, dtype=bool), 0.0\n",
    "\n",
    "def compute_vegetation_indices(bands: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Compute vegetation indices based on configuration\"\"\"\n",
    "    if not CONFIG['enable_vegetation_indices']:\n",
    "        return {}\n",
    "    \n",
    "    indices = {}\n",
    "    eps = 1e-8\n",
    "    \n",
    "    b02, b03, b04, b08, b11 = [bands.get(b) for b in ['B02', 'B03', 'B04', 'B08', 'B11']]\n",
    "    \n",
    "    try:\n",
    "        # Vectorized operations for efficiency\n",
    "        if b08 is not None and b04 is not None:\n",
    "            # EVI - Enhanced Vegetation Index\n",
    "            if b02 is not None:\n",
    "                evi = 2.5 * (b08 - b04) / (b08 + 6 * b04 - 7.5 * b02 + 1 + eps)\n",
    "                indices['EVI'] = np.clip(evi, -1, 1)\n",
    "            \n",
    "            # GNDVI - Green Normalized Difference Vegetation Index\n",
    "            if b03 is not None:\n",
    "                gndvi = (b08 - b03) / (b08 + b03 + eps)\n",
    "                indices['GNDVI'] = np.clip(gndvi, -1, 1)\n",
    "            \n",
    "            # BSI - Bare Soil Index\n",
    "            if b11 is not None and b02 is not None:\n",
    "                bsi = ((b11 + b04) - (b08 + b02)) / ((b11 + b04) + (b08 + b02) + eps)\n",
    "                indices['BSI'] = np.clip(bsi, -1, 1)\n",
    "            \n",
    "            # MSAVI2 - Modified Soil Adjusted Vegetation Index\n",
    "            sqrt_term = np.sqrt(np.maximum((2 * b08 + 1)**2 - 8 * (b08 - b04), 0))\n",
    "            msavi2 = (2 * b08 + 1 - sqrt_term) / 2\n",
    "            indices['MSAVI2'] = np.clip(msavi2, -1, 1)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def gap_fill(data: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Gap filling with configurable method\"\"\"\n",
    "    if not np.any(mask) or CONFIG['skip_quality_checks']:\n",
    "        return data\n",
    "    \n",
    "    try:\n",
    "        if CONFIG['enable_idw_interpolation']:\n",
    "            # IDW interpolation (higher quality)\n",
    "            result = data.copy()\n",
    "            missing_coords = np.where(mask)\n",
    "            h, w = data.shape\n",
    "            radius = 12  # IDW radius\n",
    "            \n",
    "            for y, x in zip(missing_coords[0], missing_coords[1]):\n",
    "                y_min, y_max = max(0, y - radius), min(h, y + radius + 1)\n",
    "                x_min, x_max = max(0, x - radius), min(w, x + radius + 1)\n",
    "                \n",
    "                local_data = data[y_min:y_max, x_min:x_max]\n",
    "                local_mask = mask[y_min:y_max, x_min:x_max]\n",
    "                valid_mask = ~local_mask & ~np.isnan(local_data)\n",
    "                \n",
    "                if np.any(valid_mask):\n",
    "                    local_y, local_x = y - y_min, x - x_min\n",
    "                    valid_coords = np.where(valid_mask)\n",
    "                    distances = np.sqrt((valid_coords[0] - local_y)**2 + (valid_coords[1] - local_x)**2)\n",
    "                    distances = np.maximum(distances, 0.1)\n",
    "                    \n",
    "                    weights = 1.0 / distances**2\n",
    "                    weights /= np.sum(weights)\n",
    "                    \n",
    "                    valid_values = local_data[valid_mask]\n",
    "                    result[y, x] = np.sum(weights * valid_values)\n",
    "                else:\n",
    "                    result[y, x] = np.nanmean(local_data)\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            # Convolution fill\n",
    "            result = data.copy()\n",
    "            kernel = np.ones((3, 3), dtype=np.float32) / 9\n",
    "            filled = ndimage.convolve(data, kernel, mode='nearest')\n",
    "            result[mask] = filled[mask]\n",
    "            return result\n",
    "        \n",
    "    except Exception:\n",
    "        return data\n",
    "\n",
    "def normalize_radiometry(target_bands: Dict, reference_bands: Dict) -> Dict:\n",
    "    \"\"\"Radiometric normalization (configurable)\"\"\"\n",
    "    if not CONFIG['enable_radiometric_normalization']:\n",
    "        return target_bands\n",
    "    \n",
    "    normalized_bands = {}\n",
    "    \n",
    "    for band_name in target_bands:\n",
    "        if band_name in reference_bands:\n",
    "            target_data = target_bands[band_name]\n",
    "            reference_data = reference_bands[band_name]\n",
    "            \n",
    "            target_valid = ~np.isnan(target_data)\n",
    "            ref_valid = ~np.isnan(reference_data)\n",
    "            \n",
    "            if np.any(target_valid) and np.any(ref_valid):\n",
    "                target_mean = np.nanmean(target_data[target_valid])\n",
    "                target_std = np.nanstd(target_data[target_valid])\n",
    "                ref_mean = np.nanmean(reference_data[ref_valid])\n",
    "                ref_std = np.nanstd(reference_data[ref_valid])\n",
    "                \n",
    "                if target_std > 0 and ref_std > 0:\n",
    "                    normalized = (target_data - target_mean) * (ref_std / target_std) + ref_mean\n",
    "                    normalized_bands[band_name] = normalized\n",
    "                else:\n",
    "                    normalized_bands[band_name] = target_data\n",
    "            else:\n",
    "                normalized_bands[band_name] = target_data\n",
    "        else:\n",
    "            normalized_bands[band_name] = target_bands[band_name]\n",
    "    \n",
    "    return normalized_bands\n",
    "\n",
    "def apply_super_resolution(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Apply super-resolution enhancement (configurable)\"\"\"\n",
    "    if not CONFIG['enable_super_resolution']:\n",
    "        return data\n",
    "    \n",
    "    try:\n",
    "        factor = 2\n",
    "        h, w = data.shape\n",
    "        new_h, new_w = h * factor, w * factor\n",
    "        \n",
    "        upscaled = cv2.resize(data, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)\n",
    "        \n",
    "        kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "        enhanced = cv2.filter2D(upscaled, -1, kernel)\n",
    "        \n",
    "        result = 0.8 * upscaled + 0.2 * enhanced\n",
    "        return result.astype(np.float32)\n",
    "        \n",
    "    except Exception:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799d5ab4-a7a3-4704-bc97-4c23cd0d89b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SatelliteProcessor:\n",
    "    def __init__(self, input_dir: str):\n",
    "        self.input_dir = Path(input_dir)\n",
    "        self.output_dir = self.input_dir / \"processed\"\n",
    "        self.cloud_free_dir = self.output_dir / \"monthly_cloud_free\"\n",
    "        self.median_dir = self.output_dir / \"monthly_median\"\n",
    "        \n",
    "        # Create output directories\n",
    "        self.cloud_free_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.median_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def discover_data(self) -> Dict[str, Dict]:\n",
    "        \"\"\"Discover satellite data with optimized scanning\"\"\"\n",
    "        print(\"ðŸ” Discovering satellite data...\")\n",
    "        \n",
    "        monthly_data = {}\n",
    "        \n",
    "        for sat_type in ['sentinel1', 'sentinel2']:\n",
    "            base_path = self.input_dir / \"raw\" / \"images\" / sat_type\n",
    "            if not base_path.exists():\n",
    "                continue\n",
    "            \n",
    "            # Use os.scandir for better performance\n",
    "            try:\n",
    "                with os.scandir(base_path) as entries:\n",
    "                    for entry in entries:\n",
    "                        if not entry.is_dir():\n",
    "                            continue\n",
    "                        \n",
    "                        name_parts = entry.name.split('_')\n",
    "                        try:\n",
    "                            if sat_type == 'sentinel1' and len(name_parts) >= 5:\n",
    "                                date_part = name_parts[4][:8]\n",
    "                            elif sat_type == 'sentinel2' and len(name_parts) >= 3:\n",
    "                                date_part = name_parts[2][:8]\n",
    "                            else:\n",
    "                                continue\n",
    "                            \n",
    "                            if len(date_part) >= 6 and date_part[:6].isdigit():\n",
    "                                year_month = f\"{date_part[:4]}_{date_part[4:6]}\"\n",
    "                                \n",
    "                                if year_month not in monthly_data:\n",
    "                                    monthly_data[year_month] = {'s1_items': [], 's2_items': []}\n",
    "                                \n",
    "                                key = 's1_items' if sat_type == 'sentinel1' else 's2_items'\n",
    "                                monthly_data[year_month][key].append(Path(entry.path))\n",
    "                        except (IndexError, ValueError):\n",
    "                            continue\n",
    "            except OSError:\n",
    "                continue\n",
    "        \n",
    "        total_items = sum(len(data['s1_items']) + len(data['s2_items']) for data in monthly_data.values())\n",
    "        print(f\"   ðŸ“Š Found {total_items} items, {len(monthly_data)} months\")\n",
    "        return monthly_data\n",
    "    \n",
    "    def load_s2_item(self, item_dir: Path) -> Tuple[str, Optional[Dict]]:\n",
    "        \"\"\"Load S2 item with configurable processing\"\"\"\n",
    "        try:\n",
    "            bands = {}\n",
    "            \n",
    "            # Efficient file discovery\n",
    "            band_files = {}\n",
    "            essential_plus_scl = ESSENTIAL_BANDS + (['SCL'] if not CONFIG['skip_quality_checks'] else [])\n",
    "            \n",
    "            # Use os.scandir for better performance\n",
    "            try:\n",
    "                with os.scandir(item_dir) as entries:\n",
    "                    for entry in entries:\n",
    "                        if entry.name.endswith(('.tif', '.tiff', '.jp2')):\n",
    "                            stem_upper = entry.name.upper()\n",
    "                            for band_name in essential_plus_scl:\n",
    "                                if band_name in stem_upper:\n",
    "                                    band_files[band_name] = Path(entry.path)\n",
    "                                    break\n",
    "                            # Early termination for speed\n",
    "                            if len(band_files) >= len(ESSENTIAL_BANDS):\n",
    "                                break\n",
    "            except OSError:\n",
    "                return item_dir.name, None\n",
    "            \n",
    "            # Load essential bands\n",
    "            reference_shape = None\n",
    "            for band_name in ESSENTIAL_BANDS:\n",
    "                if band_name in band_files:\n",
    "                    data = read_raster(band_files[band_name])\n",
    "                    if data is not None:\n",
    "                        if reference_shape is None:\n",
    "                            reference_shape = data.shape\n",
    "                        elif data.shape != reference_shape:\n",
    "                            data = cv2.resize(data, (reference_shape[1], reference_shape[0]), \n",
    "                                            interpolation=cv2.INTER_NEAREST)\n",
    "                        bands[band_name] = data\n",
    "            \n",
    "            if len(bands) < 3:\n",
    "                return item_dir.name, None\n",
    "            \n",
    "            # Vegetation indices (if enabled)\n",
    "            if CONFIG['enable_vegetation_indices']:\n",
    "                indices = compute_vegetation_indices(bands)\n",
    "                bands.update(indices)\n",
    "                \n",
    "                # Limit total bands for performance\n",
    "                if len(bands) > CONFIG['max_bands_per_composite']:\n",
    "                    priority_bands = ['B04', 'B08', 'B11', 'EVI', 'GNDVI', 'B02', 'B03']\n",
    "                    kept_bands = {}\n",
    "                    for band_name in priority_bands:\n",
    "                        if band_name in bands:\n",
    "                            kept_bands[band_name] = bands[band_name]\n",
    "                        if len(kept_bands) >= CONFIG['max_bands_per_composite']:\n",
    "                            break\n",
    "                    bands = kept_bands\n",
    "            \n",
    "            # Cloud detection\n",
    "            cloud_mask = None\n",
    "            cloud_cover_percent = 0\n",
    "            \n",
    "            if not CONFIG['skip_quality_checks'] and 'SCL' in band_files:\n",
    "                scl_data = read_raster(band_files['SCL'])\n",
    "                if scl_data is not None:\n",
    "                    if scl_data.shape != reference_shape:\n",
    "                        scl_data = cv2.resize(scl_data, (reference_shape[1], reference_shape[0]), \n",
    "                                            interpolation=cv2.INTER_NEAREST)\n",
    "                    \n",
    "                    cloud_mask, _ = detect_clouds(scl_data, bands if CONFIG['enable_advanced_cloud_detection'] else None)\n",
    "                    cloud_cover_percent = np.sum(cloud_mask) / cloud_mask.size * 100\n",
    "            \n",
    "            return item_dir.name, {\n",
    "                'bands': bands,\n",
    "                'cloud_mask': cloud_mask,\n",
    "                'cloud_cover_percent': cloud_cover_percent\n",
    "            }\n",
    "            \n",
    "        except Exception:\n",
    "            return item_dir.name, None\n",
    "    \n",
    "    def load_s1_item(self, item_dir: Path) -> Tuple[str, Optional[Dict]]:\n",
    "        \"\"\"Load S1 item\"\"\"\n",
    "        try:\n",
    "            # Scan for VV/VH files\n",
    "            vv_file = vh_file = None\n",
    "            \n",
    "            try:\n",
    "                with os.scandir(item_dir) as entries:\n",
    "                    for entry in entries:\n",
    "                        if entry.name.endswith(('.tif', '.tiff')):\n",
    "                            stem_upper = entry.name.upper()\n",
    "                            if 'VV' in stem_upper and vv_file is None:\n",
    "                                vv_file = Path(entry.path)\n",
    "                            elif 'VH' in stem_upper and vh_file is None:\n",
    "                                vh_file = Path(entry.path)\n",
    "                            \n",
    "                            if vv_file and vh_file:\n",
    "                                break\n",
    "            except OSError:\n",
    "                return item_dir.name, None\n",
    "            \n",
    "            if not vv_file or not vh_file:\n",
    "                return item_dir.name, None\n",
    "            \n",
    "            # Load data\n",
    "            vv_data = read_raster(vv_file)\n",
    "            vh_data = read_raster(vh_file)\n",
    "            \n",
    "            if vv_data is None or vh_data is None:\n",
    "                return item_dir.name, None\n",
    "            \n",
    "            # Shape matching\n",
    "            if vv_data.shape != vh_data.shape:\n",
    "                min_h = min(vv_data.shape[0], vh_data.shape[0])\n",
    "                min_w = min(vv_data.shape[1], vh_data.shape[1])\n",
    "                vv_data = vv_data[:min_h, :min_w]\n",
    "                vh_data = vh_data[:min_h, :min_w]\n",
    "            \n",
    "            # dB conversion with clipping\n",
    "            vv_db = np.clip(10 * np.log10(np.maximum(vv_data, 1e-10)), -30, 5)\n",
    "            vh_db = np.clip(10 * np.log10(np.maximum(vh_data, 1e-10)), -30, 5)\n",
    "            \n",
    "            return item_dir.name, {\n",
    "                'bands': {'VV': vv_db, 'VH': vh_db}\n",
    "            }\n",
    "            \n",
    "        except Exception:\n",
    "            return item_dir.name, None\n",
    "    \n",
    "    def create_cloud_free_composite(self, s2_items: List[Tuple]) -> Optional[Dict]:\n",
    "        \"\"\"Create cloud-free composite with configurable processing\"\"\"\n",
    "        valid_items = [(item_id, data) for item_id, data in s2_items if data is not None]\n",
    "        \n",
    "        if not valid_items:\n",
    "            return None\n",
    "        \n",
    "        # Reference selection\n",
    "        if CONFIG['skip_quality_checks']:\n",
    "            reference_item = valid_items[0]\n",
    "        else:\n",
    "            reference_item = min(valid_items, \n",
    "                               key=lambda x: x[1].get('cloud_cover_percent', 100))\n",
    "        \n",
    "        ref_id, ref_data = reference_item\n",
    "        ref_bands = ref_data['bands'].copy()\n",
    "        \n",
    "        # Apply cloud mask\n",
    "        if not CONFIG['skip_quality_checks'] and ref_data.get('cloud_mask') is not None:\n",
    "            cloud_mask = ref_data['cloud_mask']\n",
    "            for band_name in ref_bands:\n",
    "                ref_bands[band_name] = np.where(cloud_mask, np.nan, ref_bands[band_name])\n",
    "        \n",
    "        composite_bands = {}\n",
    "        \n",
    "        for band_name, ref_band in ref_bands.items():\n",
    "            result = ref_band.copy()\n",
    "            \n",
    "            # Gap filling\n",
    "            if not CONFIG['skip_quality_checks']:\n",
    "                missing_mask = np.isnan(result)\n",
    "                if np.any(missing_mask):\n",
    "                    # Create median composite from all valid images\n",
    "                    valid_stack = []\n",
    "                    for item_id, data in valid_items:\n",
    "                        if (item_id != ref_id and 'bands' in data and \n",
    "                            band_name in data['bands'] and\n",
    "                            data['bands'][band_name].shape == result.shape):\n",
    "                            \n",
    "                            fill_data = data['bands'][band_name].copy()\n",
    "                            \n",
    "                            # Apply cloud mask to fill data\n",
    "                            if data.get('cloud_mask') is not None:\n",
    "                                fill_data = np.where(data['cloud_mask'], np.nan, fill_data)\n",
    "                            \n",
    "                            if np.any(~np.isnan(fill_data)):\n",
    "                                valid_stack.append(fill_data)\n",
    "                    \n",
    "                    if valid_stack:\n",
    "                        # Ensure all arrays have the same shape before creating median\n",
    "                        ref_shape = result.shape\n",
    "                        aligned_stack = []\n",
    "                        for fill_data in valid_stack:\n",
    "                            if fill_data.shape == ref_shape:\n",
    "                                aligned_stack.append(fill_data)\n",
    "                            else:\n",
    "                                # Resize to match reference shape\n",
    "                                resized = cv2.resize(fill_data, (ref_shape[1], ref_shape[0]), \n",
    "                                                   interpolation=cv2.INTER_LINEAR)\n",
    "                                aligned_stack.append(resized)\n",
    "                        \n",
    "                        if aligned_stack:\n",
    "                            median_composite = np.nanmedian(aligned_stack, axis=0)\n",
    "                        \n",
    "                        # Apply radiometric normalization if enabled\n",
    "                        if CONFIG['enable_radiometric_normalization']:\n",
    "                            normalized_composite = normalize_radiometry(\n",
    "                                {band_name: median_composite}, \n",
    "                                {band_name: ref_band}\n",
    "                            )[band_name]\n",
    "                        else:\n",
    "                            normalized_composite = median_composite\n",
    "                        \n",
    "                        # Fill gaps\n",
    "                        fill_mask = missing_mask & ~np.isnan(normalized_composite)\n",
    "                        result[fill_mask] = normalized_composite[fill_mask]\n",
    "                        missing_mask = np.isnan(result)\n",
    "                    \n",
    "                    # Apply gap filling method based on configuration\n",
    "                    if np.any(missing_mask):\n",
    "                        result = gap_fill(result, missing_mask)\n",
    "            \n",
    "            composite_bands[band_name] = result\n",
    "        \n",
    "        if len(composite_bands) >= 3:\n",
    "            return {\n",
    "                'bands': composite_bands,\n",
    "                'metadata': {\n",
    "                    'reference_image': ref_id,\n",
    "                    'num_source_images': len(valid_items),\n",
    "                    'processing_method': 'configurable_cloud_free'\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def create_median_composite(self, s2_items: List[Tuple], s1_items: List[Tuple]) -> Optional[Dict]:\n",
    "        \"\"\"Create median composite with configurable processing\"\"\"\n",
    "        all_bands = {}\n",
    "        \n",
    "        # Process S2 data\n",
    "        if s2_items:\n",
    "            valid_s2 = [(item_id, data) for item_id, data in s2_items if data is not None]\n",
    "            \n",
    "            if valid_s2:\n",
    "                # Get reference shape\n",
    "                reference_shape = None\n",
    "                for _, data in valid_s2:\n",
    "                    if 'bands' in data and data['bands']:\n",
    "                        reference_shape = next(iter(data['bands'].values())).shape\n",
    "                        break\n",
    "                \n",
    "                if reference_shape:\n",
    "                    target_bands = ESSENTIAL_BANDS + VEGETATION_INDICES\n",
    "                    \n",
    "                    for band_name in target_bands:\n",
    "                        band_stack = []\n",
    "                        \n",
    "                        for _, data in valid_s2:\n",
    "                            if ('bands' in data and band_name in data['bands']):\n",
    "                                band_data = data['bands'][band_name]\n",
    "                                \n",
    "                                # Apply cloud mask\n",
    "                                if not CONFIG['skip_quality_checks'] and data.get('cloud_mask') is not None:\n",
    "                                    band_data = np.where(data['cloud_mask'], np.nan, band_data)\n",
    "                                \n",
    "                                if (band_data.shape == reference_shape and \n",
    "                                    (CONFIG['skip_quality_checks'] or np.any(~np.isnan(band_data)))):\n",
    "                                    band_stack.append(band_data)\n",
    "                        \n",
    "                        if len(band_stack) >= 1:\n",
    "                            if len(band_stack) == 1:\n",
    "                                result = band_stack[0].astype(np.float32)\n",
    "                            else:\n",
    "                                # Ensure all arrays have the same shape before stacking\n",
    "                                ref_shape = band_stack[0].shape\n",
    "                                aligned_stack = []\n",
    "                                for band_data in band_stack:\n",
    "                                    if band_data.shape == ref_shape:\n",
    "                                        aligned_stack.append(band_data)\n",
    "                                    else:\n",
    "                                        # Resize to match reference shape\n",
    "                                        resized = cv2.resize(band_data, (ref_shape[1], ref_shape[0]), \n",
    "                                                           interpolation=cv2.INTER_LINEAR)\n",
    "                                        aligned_stack.append(resized)\n",
    "                                \n",
    "                                if len(aligned_stack) >= 1:\n",
    "                                    result = np.nanmedian(aligned_stack, axis=0).astype(np.float32)\n",
    "                                else:\n",
    "                                    continue\n",
    "                            \n",
    "                            # Apply super-resolution if enabled\n",
    "                            if CONFIG['enable_super_resolution']:\n",
    "                                result = apply_super_resolution(result)\n",
    "                            \n",
    "                            all_bands[band_name] = result\n",
    "        \n",
    "        # Process S1 data\n",
    "        if s1_items:\n",
    "            valid_s1 = [(item_id, data) for item_id, data in s1_items if data is not None]\n",
    "            \n",
    "            for band_name in S1_BANDS:\n",
    "                band_stack = []\n",
    "                \n",
    "                for _, data in valid_s1:\n",
    "                    if ('bands' in data and band_name in data['bands']):\n",
    "                        band_data = data['bands'][band_name]\n",
    "                        if CONFIG['skip_quality_checks'] or np.any(~np.isnan(band_data)):\n",
    "                            band_stack.append(band_data)\n",
    "                \n",
    "                if len(band_stack) >= 1:\n",
    "                    if len(band_stack) == 1:\n",
    "                        result = band_stack[0].astype(np.float32)\n",
    "                    else:\n",
    "                        # Ensure all S1 arrays have the same shape\n",
    "                        ref_shape = band_stack[0].shape\n",
    "                        aligned_stack = []\n",
    "                        for band_data in band_stack:\n",
    "                            if band_data.shape == ref_shape:\n",
    "                                aligned_stack.append(band_data)\n",
    "                            else:\n",
    "                                # Resize S1 data to match reference shape\n",
    "                                resized = cv2.resize(band_data, (ref_shape[1], ref_shape[0]), \n",
    "                                                   interpolation=cv2.INTER_LINEAR)\n",
    "                                aligned_stack.append(resized)\n",
    "                        \n",
    "                        if len(aligned_stack) >= 1:\n",
    "                            result = np.nanmedian(aligned_stack, axis=0).astype(np.float32)\n",
    "                        else:\n",
    "                            continue\n",
    "                    \n",
    "                    # Apply super-resolution if enabled\n",
    "                    if CONFIG['enable_super_resolution']:\n",
    "                        result = apply_super_resolution(result)\n",
    "                    \n",
    "                    all_bands[band_name] = result\n",
    "        \n",
    "        if len(all_bands) >= 3:\n",
    "            return {\n",
    "                'bands': all_bands,\n",
    "                'metadata': {\n",
    "                    'num_s2_images': len([x for x in s2_items if x[1] is not None]),\n",
    "                    'num_s1_images': len([x for x in s1_items if x[1] is not None]),\n",
    "                    'processing_method': 'configurable_median',\n",
    "                    'super_resolution_applied': CONFIG['enable_super_resolution']\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def save_composite(self, data: Dict, output_path: Path) -> bool:\n",
    "        \"\"\"Save composite with configurable compression\"\"\"\n",
    "        try:\n",
    "            bands = data['bands']\n",
    "            band_names = sorted(bands.keys())\n",
    "            first_band = list(bands.values())[0]\n",
    "            height, width = first_band.shape\n",
    "            \n",
    "            # Profile based on configuration\n",
    "            if CONFIG['enable_super_resolution'] or len(band_names) > 6:\n",
    "                # Use compression for larger files\n",
    "                profile = {\n",
    "                    'driver': 'GTiff',\n",
    "                    'count': len(band_names),\n",
    "                    'height': height,\n",
    "                    'width': width,\n",
    "                    'dtype': 'float32',\n",
    "                    'compress': 'lzw',\n",
    "                    'predictor': 2,\n",
    "                    'tiled': True,\n",
    "                    'blockxsize': 512,\n",
    "                    'blockysize': 512\n",
    "                }\n",
    "            else:\n",
    "                # Optimized profile for speed\n",
    "                profile = {\n",
    "                    'driver': 'GTiff',\n",
    "                    'count': len(band_names),\n",
    "                    'height': height,\n",
    "                    'width': width,\n",
    "                    'dtype': 'float32',\n",
    "                    'compress': 'none',\n",
    "                    'tiled': False,\n",
    "                    'interleave': 'pixel'\n",
    "                }\n",
    "            \n",
    "            output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "                # Write all bands efficiently\n",
    "                if CONFIG['skip_quality_checks']:\n",
    "                    # Batch write for speed - ensure all bands have same shape\n",
    "                    first_shape = list(bands.values())[0].shape\n",
    "                    aligned_bands = []\n",
    "                    for name in band_names:\n",
    "                        band_data = bands[name]\n",
    "                        if band_data.shape == first_shape:\n",
    "                            aligned_bands.append(band_data)\n",
    "                        else:\n",
    "                            # Resize to match first band shape\n",
    "                            resized = cv2.resize(band_data, (first_shape[1], first_shape[0]), \n",
    "                                               interpolation=cv2.INTER_LINEAR)\n",
    "                            aligned_bands.append(resized)\n",
    "                    \n",
    "                    band_array = np.stack(aligned_bands)\n",
    "                    dst.write(band_array)\n",
    "                else:\n",
    "                    # Individual band writing with descriptions\n",
    "                    first_shape = list(bands.values())[0].shape\n",
    "                    for i, band_name in enumerate(band_names, 1):\n",
    "                        band_data = bands[band_name]\n",
    "                        if band_data.shape != first_shape:\n",
    "                            # Resize to match first band shape\n",
    "                            band_data = cv2.resize(band_data, (first_shape[1], first_shape[0]), \n",
    "                                                 interpolation=cv2.INTER_LINEAR)\n",
    "                        dst.write(band_data.astype(np.float32), i)\n",
    "                        dst.set_band_description(i, band_name)\n",
    "                \n",
    "                # Add metadata\n",
    "                if not CONFIG['skip_quality_checks']:\n",
    "                    dst.update_tags(**data['metadata'])\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception:\n",
    "            return False\n",
    "    \n",
    "    def check_existing_outputs(self, year_month: str) -> Tuple[bool, bool]:\n",
    "        \"\"\"Check if outputs already exist\"\"\"\n",
    "        cloud_free_path = self.cloud_free_dir / f\"{year_month}_cloud_free.tif\"\n",
    "        median_path = self.median_dir / f\"{year_month}_median.tif\"\n",
    "        return cloud_free_path.exists(), median_path.exists()\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"Main processing pipeline with configurable features\"\"\"\n",
    "        print(f\"ðŸš€ Satellite Processor\")\n",
    "        \n",
    "        # Show optimization status\n",
    "        optimizations = []\n",
    "        if CONFIG['downsample_factor'] > 1:\n",
    "            optimizations.append(f\"Downsampling: {CONFIG['downsample_factor']}x\")\n",
    "        if CONFIG['skip_quality_checks']:\n",
    "            optimizations.append(\"Quality checks disabled\")\n",
    "        if CONFIG['max_bands_per_composite'] < 10:\n",
    "            optimizations.append(f\"Max bands: {CONFIG['max_bands_per_composite']}\")\n",
    "        \n",
    "        enabled_features = []\n",
    "        if CONFIG['enable_vegetation_indices']:\n",
    "            enabled_features.append(\"Vegetation Indices\")\n",
    "        if CONFIG['enable_advanced_cloud_detection']:\n",
    "            enabled_features.append(\"Advanced Cloud Detection\")\n",
    "        if CONFIG['enable_idw_interpolation']:\n",
    "            enabled_features.append(\"IDW Interpolation\")\n",
    "        if CONFIG['enable_radiometric_normalization']:\n",
    "            enabled_features.append(\"Radiometric Normalization\")\n",
    "        if CONFIG['enable_super_resolution']:\n",
    "            enabled_features.append(\"Super Resolution\")\n",
    "        \n",
    "        if optimizations:\n",
    "            print(f\"   âš¡ Optimizations: {', '.join(optimizations)}\")\n",
    "        if enabled_features:\n",
    "            print(f\"   âœ… Enabled: {', '.join(enabled_features)}\")\n",
    "        else:\n",
    "            print(f\"   âš¡ Speed mode: All advanced features disabled\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        monthly_data = self.discover_data()\n",
    "        \n",
    "        if not monthly_data:\n",
    "            print(\"âŒ No data found\")\n",
    "            return\n",
    "        \n",
    "        processed_count = 0\n",
    "        \n",
    "        for month_idx, year_month in enumerate(sorted(monthly_data.keys()), 1):\n",
    "            print(f\"ðŸ“… {month_idx}/{len(monthly_data)}: {year_month}\", end=' ')\n",
    "            \n",
    "            # Check existing outputs first - before any data loading\n",
    "            cloud_free_path = self.cloud_free_dir / f\"{year_month}_cloud_free.tif\"\n",
    "            median_path = self.median_dir / f\"{year_month}_median.tif\"\n",
    "            cloud_free_exists = cloud_free_path.exists()\n",
    "            median_exists = median_path.exists()\n",
    "            \n",
    "            if cloud_free_exists and median_exists:\n",
    "                print(\"(both exist)\")\n",
    "                continue\n",
    "            \n",
    "            month_data = monthly_data[year_month]\n",
    "            s2_count = len(month_data['s2_items'])\n",
    "            \n",
    "            if s2_count == 0:\n",
    "                print(\"(no S2)\")\n",
    "                continue\n",
    "            \n",
    "            # Only load data if we need to create at least one composite\n",
    "            max_s2_items = min(s2_count, 10) if CONFIG['skip_quality_checks'] else s2_count\n",
    "            max_s1_items = min(len(month_data['s1_items']), 5) if CONFIG['skip_quality_checks'] else len(month_data['s1_items'])\n",
    "            \n",
    "            # Load S2 data (always needed)\n",
    "            with ThreadPoolExecutor(max_workers=CONFIG['io_workers']) as executor:\n",
    "                s2_futures = [executor.submit(self.load_s2_item, item_dir) \n",
    "                             for item_dir in month_data['s2_items'][:max_s2_items]]\n",
    "                s2_items = [f.result() for f in s2_futures]\n",
    "            \n",
    "            valid_s2 = sum(1 for _, data in s2_items if data is not None)\n",
    "            if valid_s2 == 0:\n",
    "                print(\"(no valid S2)\")\n",
    "                continue\n",
    "            \n",
    "            # Load S1 data only if we need median composite\n",
    "            s1_items = []\n",
    "            if not median_exists:\n",
    "                with ThreadPoolExecutor(max_workers=CONFIG['io_workers']) as executor:\n",
    "                    s1_futures = [executor.submit(self.load_s1_item, item_dir) \n",
    "                                 for item_dir in month_data['s1_items'][:max_s1_items]]\n",
    "                    s1_items = [f.result() for f in s1_futures]\n",
    "            \n",
    "            # Create and save composites\n",
    "            success_count = 0\n",
    "            \n",
    "            if not cloud_free_exists:\n",
    "                cf_composite = self.create_cloud_free_composite(s2_items)\n",
    "                if cf_composite and self.save_composite(cf_composite, cloud_free_path):\n",
    "                    success_count += 1\n",
    "            else:\n",
    "                success_count += 1\n",
    "            \n",
    "            if not median_exists:\n",
    "                med_composite = self.create_median_composite(s2_items, s1_items)\n",
    "                if med_composite and self.save_composite(med_composite, median_path):\n",
    "                    success_count += 1\n",
    "            else:\n",
    "                success_count += 1\n",
    "            \n",
    "            print(f\"({success_count}/2 saved)\")\n",
    "            processed_count += 1\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del s2_items, s1_items\n",
    "            if processed_count % 5 == 0:  # Less frequent GC for speed\n",
    "                gc.collect()\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\nðŸŽ‰ Processing complete!\")\n",
    "        print(f\"   â±ï¸  Total time: {total_time//60:.0f}m {total_time%60:.1f}s\")\n",
    "        print(f\"   ðŸ“Š Processed: {processed_count} months\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6e768-6f4b-495b-9cdb-24177d28dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    INPUT_DIR = \"satellite_data\"\n",
    "    \n",
    "    try:\n",
    "        processor = SatelliteProcessor(INPUT_DIR)\n",
    "        processor.process()\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nâš ï¸  Interrupted\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c3c62-2c97-410c-bd73-2debcd919a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
